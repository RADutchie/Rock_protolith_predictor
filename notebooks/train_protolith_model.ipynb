{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Protolith classification model training script.\n",
    "\n",
    "Model uses major element geochemistry and a balanced random forrest algorithm to discriminate sedimentary from ignous protoliths. \n",
    "\n",
    "Scripts to create and train final model pipeline on complete dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Paths to training dataset and model output location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to complete training dataset \n",
    "in_path = r'D:\\Python ML\\SA_Geology_protolith_predictions\\data\\processed\\classifier_data_2019-02-26_combined.csv'\n",
    "\n",
    "#Path to location to save model\n",
    "out_path = r'D:\\Python ML\\SA_Geology_protolith_predictions\\models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to load the protolith model training data from file. Data is already in Atchison simplex form.\n",
    "    Parameters:\n",
    "        path: path to training data as string\n",
    "    returns:\n",
    "        training dataset X_train, y_train\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    try:\n",
    "        if p.exists and p.suffix == '.csv':\n",
    "            df = pd.read_csv(p) #training set\n",
    "    except:\n",
    "        print('could not read training data file')\n",
    "\n",
    "    #create raw X training set and binary labeled y array\n",
    "    X_train, y_train = df[['SiO2','TiO2','Al2O3','FeOT','MgO','CaO','Na2O','K2O','P2O5']], df[['ROCK_GROUP']].replace(['igneous','sedimentary'],[0,1])\n",
    "\n",
    "    return X_train, y_train  \n",
    "\n",
    "def train_model(X: pd.DataFrame, y: pd.DataFrame) -> sklearn.pipeline.Pipeline:\n",
    "    \"\"\"\n",
    "    Function to train the protolith classification model.\n",
    "    Paramaters:\n",
    "        X: training data, default X_train\n",
    "        y: training labels, default y_train\n",
    "    returns\n",
    "        trained model pipeline\n",
    "    \"\"\"\n",
    "    # Create a pipeline to scale and train a calibrated balanced random forrest classifier\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=101)\n",
    "\n",
    "    clsf_pipe = Pipeline([('sc', StandardScaler()),\n",
    "                ('classifier', CalibratedClassifierCV(base_estimator = BalancedRandomForestClassifier(n_estimators= 50, max_depth=15, min_samples_leaf= 1, min_samples_split= 2, max_features= 'sqrt',sampling_strategy='not minority', n_jobs= -1, random_state= 101), method= 'sigmoid', cv=cv))])\n",
    "    # fit the model\n",
    "    model_pipe = clsf_pipe.fit(X, y)\n",
    "    \n",
    "    return model_pipe\n",
    "\n",
    "def save_model(model, name: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Function to save trained protolith classification model.\n",
    "    Paramaters:\n",
    "        model: trained sklearn model pipe\n",
    "        name: str. File name (will be appended with date and time)\n",
    "        out_path: str. path to export file to\n",
    "    returns\n",
    "        joblib file\n",
    "    \"\"\"\n",
    "    now = datetime.now().strftime(\"%Y-%M-%d-%H-%M\")\n",
    "    file_name = f'{name}_{now}.joblib' \n",
    "    p = Path(out_path) / file_name\n",
    "    joblib.dump(model, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to load data, train model and output saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_training_data(in_path)\n",
    "\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "save_model(model, 'Model50_15_full', out_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitpycaretconda04d964cf706c4fb9995a657b6f62f611",
   "display_name": "Python 3.6.10 64-bit ('pycaret': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}